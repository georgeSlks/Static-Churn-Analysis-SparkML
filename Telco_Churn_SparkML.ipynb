{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJStH0bANykTlLlujSa8DZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOXD82iuApgU"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.5.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "19rVgnAeSPVn",
        "outputId": "c65edcf3-8992-46b8-9036-4124fb413d13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.5.0\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark==3.5.0) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425346 sha256=8694390ea85e1d894857a9e54d78537e130bd6afd2bb1c9f20212a37fcad1dcd\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/40/20/65eefe766118e0a8f8e385cc3ed6e9eb7241c7e51cfc04c51a\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.5.1\n",
            "    Uninstalling pyspark-3.5.1:\n",
            "      Successfully uninstalled pyspark-3.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 0.8.3 requires pyspark[connect]~=3.5.1, but you have pyspark 3.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TelcoChurn\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "collapsed": true,
        "id": "O_HFe87qSzOm",
        "outputId": "93344499-dd52-427d-bcfc-3249d5decf85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x77fe733e80b0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://81bdf9d23978:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>TelcoChurn</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UVG9WXz2TWFv",
        "outputId": "41efdf95-9c78-4d2c-b861-2b31da0ec66b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drive CSV path\n",
        "file_path = \"/content/drive/MyDrive/datasets/telcoDataset/telcoDataset.csv\"\n",
        "\n",
        "# PySpark DataFrame load\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# 5 first rows\n",
        "df.show(5)\n",
        "\n",
        "# schema\n",
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5e_30dbnTqDV",
        "outputId": "02ad09a3-7682-45b9-9add-1e76b3674700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+--------------------+--------------+------------+-----+\n",
            "|customerID|gender|SeniorCitizen|Partner|Dependents|tenure|PhoneService|   MultipleLines|InternetService|OnlineSecurity|OnlineBackup|DeviceProtection|TechSupport|StreamingTV|StreamingMovies|      Contract|PaperlessBilling|       PaymentMethod|MonthlyCharges|TotalCharges|Churn|\n",
            "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+--------------------+--------------+------------+-----+\n",
            "|7590-VHVEG|Female|            0|    Yes|        No|     1|          No|No phone service|            DSL|            No|         Yes|              No|         No|         No|             No|Month-to-month|             Yes|    Electronic check|         29.85|       29.85|   No|\n",
            "|5575-GNVDE|  Male|            0|     No|        No|    34|         Yes|              No|            DSL|           Yes|          No|             Yes|         No|         No|             No|      One year|              No|        Mailed check|         56.95|      1889.5|   No|\n",
            "|3668-QPYBK|  Male|            0|     No|        No|     2|         Yes|              No|            DSL|           Yes|         Yes|              No|         No|         No|             No|Month-to-month|             Yes|        Mailed check|         53.85|      108.15|  Yes|\n",
            "|7795-CFOCW|  Male|            0|     No|        No|    45|          No|No phone service|            DSL|           Yes|          No|             Yes|        Yes|         No|             No|      One year|              No|Bank transfer (au...|          42.3|     1840.75|   No|\n",
            "|9237-HQITU|Female|            0|     No|        No|     2|         Yes|              No|    Fiber optic|            No|          No|              No|         No|         No|             No|Month-to-month|             Yes|    Electronic check|          70.7|      151.65|  Yes|\n",
            "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+--------------------+--------------+------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- customerID: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- SeniorCitizen: integer (nullable = true)\n",
            " |-- Partner: string (nullable = true)\n",
            " |-- Dependents: string (nullable = true)\n",
            " |-- tenure: integer (nullable = true)\n",
            " |-- PhoneService: string (nullable = true)\n",
            " |-- MultipleLines: string (nullable = true)\n",
            " |-- InternetService: string (nullable = true)\n",
            " |-- OnlineSecurity: string (nullable = true)\n",
            " |-- OnlineBackup: string (nullable = true)\n",
            " |-- DeviceProtection: string (nullable = true)\n",
            " |-- TechSupport: string (nullable = true)\n",
            " |-- StreamingTV: string (nullable = true)\n",
            " |-- StreamingMovies: string (nullable = true)\n",
            " |-- Contract: string (nullable = true)\n",
            " |-- PaperlessBilling: string (nullable = true)\n",
            " |-- PaymentMethod: string (nullable = true)\n",
            " |-- MonthlyCharges: double (nullable = true)\n",
            " |-- TotalCharges: string (nullable = true)\n",
            " |-- Churn: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Handle empty or non-numeric values\n",
        "df = df.withColumn(\"TotalCharges\",\n",
        "                   col(\"TotalCharges\").cast(\"double\"))\n",
        "\n",
        "# Check for null values in the TotalCharges column\n",
        "df.filter(col(\"TotalCharges\").isNull()).count()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJBOTEEmVdgm",
        "outputId": "76b0bcb4-0e20-4e36-857b-d577c2d6d220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "\n",
        "# types transformation\n",
        "# TotalCharges: string → double\n",
        "df = df.withColumn(\"TotalCharges\", col(\"TotalCharges\").cast(\"double\"))\n",
        "\n",
        "# Check for missing values in TotalCharges\n",
        "null_count = df.filter(col(\"TotalCharges\").isNull()).count()\n",
        "print(f\"Missing values in TotalCharges: {null_count}\")\n",
        "\n",
        "# if nulls -> fill with median value\n",
        "if null_count > 0:\n",
        "    median_total = df.approxQuantile(\"TotalCharges\", [0.5], 0.0)[0]\n",
        "    df = df.na.fill({\"TotalCharges\": median_total})\n",
        "\n",
        "# target transformation\n",
        "# Churn Yes/No → label 1/0\n",
        "churn_indexer = StringIndexer(inputCol=\"Churn\", outputCol=\"label\")\n",
        "df = churn_indexer.fit(df).transform(df)\n",
        "\n",
        "# delete customer id and churn collumns\n",
        "df = df.drop(\"customerID\", \"Churn\")\n",
        "\n",
        "# Encoding categorical features\n",
        "categorical_cols = [\"gender\", \"Partner\", \"Dependents\", \"PhoneService\", \"MultipleLines\",\n",
        "                    \"InternetService\", \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\",\n",
        "                    \"TechSupport\", \"StreamingTV\", \"StreamingMovies\", \"Contract\",\n",
        "                    \"PaperlessBilling\", \"PaymentMethod\"]\n",
        "\n",
        "# StringIndexer for all categorical collumns\n",
        "indexers = [StringIndexer(inputCol=col_name, outputCol=col_name+\"_Index\")\n",
        "            for col_name in categorical_cols]\n",
        "\n",
        "# OneHotEncoder for all indexed collumns\n",
        "encoder = OneHotEncoder(inputCols=[idx.getOutputCol() for idx in indexers],\n",
        "                        outputCols=[col_name+\"_OHE\" for col_name in categorical_cols])\n",
        "\n",
        "# indexers train\n",
        "from pyspark.ml import Pipeline\n",
        "pipeline = Pipeline(stages=indexers + [encoder])\n",
        "df = pipeline.fit(df).transform(df)\n",
        "\n",
        "# show results\n",
        "df.select([\"label\", \"TotalCharges\", \"MonthlyCharges\"] + [col+\"_OHE\" for col in categorical_cols]).show(5, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rKCXLEylYYyL",
        "outputId": "a69fe1ae-a97a-4b9e-c200-0e81daf6d8d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in TotalCharges: 11\n",
            "+-----+------------+--------------+-------------+-------------+--------------+----------------+-----------------+-------------------+------------------+----------------+--------------------+---------------+---------------+-------------------+-------------+--------------------+-----------------+\n",
            "|label|TotalCharges|MonthlyCharges|gender_OHE   |Partner_OHE  |Dependents_OHE|PhoneService_OHE|MultipleLines_OHE|InternetService_OHE|OnlineSecurity_OHE|OnlineBackup_OHE|DeviceProtection_OHE|TechSupport_OHE|StreamingTV_OHE|StreamingMovies_OHE|Contract_OHE |PaperlessBilling_OHE|PaymentMethod_OHE|\n",
            "+-----+------------+--------------+-------------+-------------+--------------+----------------+-----------------+-------------------+------------------+----------------+--------------------+---------------+---------------+-------------------+-------------+--------------------+-----------------+\n",
            "|0.0  |29.85       |29.85         |(1,[],[])    |(1,[],[])    |(1,[0],[1.0]) |(1,[],[])       |(2,[],[])        |(2,[1],[1.0])      |(2,[0],[1.0])     |(2,[1],[1.0])   |(2,[0],[1.0])       |(2,[0],[1.0])  |(2,[0],[1.0])  |(2,[0],[1.0])      |(2,[0],[1.0])|(1,[0],[1.0])       |(3,[0],[1.0])    |\n",
            "|0.0  |1889.5      |56.95         |(1,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0]) |(1,[0],[1.0])   |(2,[0],[1.0])    |(2,[1],[1.0])      |(2,[1],[1.0])     |(2,[0],[1.0])   |(2,[1],[1.0])       |(2,[0],[1.0])  |(2,[0],[1.0])  |(2,[0],[1.0])      |(2,[],[])    |(1,[],[])           |(3,[1],[1.0])    |\n",
            "|1.0  |108.15      |53.85         |(1,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0]) |(1,[0],[1.0])   |(2,[0],[1.0])    |(2,[1],[1.0])      |(2,[1],[1.0])     |(2,[1],[1.0])   |(2,[0],[1.0])       |(2,[0],[1.0])  |(2,[0],[1.0])  |(2,[0],[1.0])      |(2,[0],[1.0])|(1,[0],[1.0])       |(3,[1],[1.0])    |\n",
            "|0.0  |1840.75     |42.3          |(1,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0]) |(1,[],[])       |(2,[],[])        |(2,[1],[1.0])      |(2,[1],[1.0])     |(2,[0],[1.0])   |(2,[1],[1.0])       |(2,[1],[1.0])  |(2,[0],[1.0])  |(2,[0],[1.0])      |(2,[],[])    |(1,[],[])           |(3,[2],[1.0])    |\n",
            "|1.0  |151.65      |70.7          |(1,[],[])    |(1,[0],[1.0])|(1,[0],[1.0]) |(1,[0],[1.0])   |(2,[0],[1.0])    |(2,[0],[1.0])      |(2,[0],[1.0])     |(2,[0],[1.0])   |(2,[0],[1.0])       |(2,[0],[1.0])  |(2,[0],[1.0])  |(2,[0],[1.0])      |(2,[0],[1.0])|(1,[0],[1.0])       |(3,[0],[1.0])    |\n",
            "+-----+------------+--------------+-------------+-------------+--------------+----------------+-----------------+-------------------+------------------+----------------+--------------------+---------------+---------------+-------------------+-------------+--------------------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Prepare list of feature columns\n",
        "numeric_cols = [\"MonthlyCharges\", \"TotalCharges\"]\n",
        "\n",
        "# One-hot encoded categorical columns\n",
        "ohe_cols = [col+\"_OHE\" for col in [\"gender\", \"Partner\", \"Dependents\", \"PhoneService\", \"MultipleLines\",\n",
        "                                   \"InternetService\", \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\",\n",
        "                                   \"TechSupport\", \"StreamingTV\", \"StreamingMovies\", \"Contract\",\n",
        "                                   \"PaperlessBilling\", \"PaymentMethod\"]]\n",
        "\n",
        "all_features = numeric_cols + ohe_cols\n",
        "\n",
        "# VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=all_features, outputCol=\"features\")\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# show results(five first labels after vectorization)\n",
        "df.select(\"label\", \"features\").show(5, truncate=False)\n",
        "\n",
        "# Train/Test Split\n",
        "train_df, test_df = df.randomSplit([0.7, 0.3], seed=42)\n",
        "print(f\"Train rows: {train_df.count()}, Test rows: {test_df.count()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "69vg6LScoN8T",
        "outputId": "43237227-a08d-4bc7-d63d-b215ab169fb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------------------------------------------------------------------------------------------------------------------------+\n",
            "|label|features                                                                                                                    |\n",
            "+-----+----------------------------------------------------------------------------------------------------------------------------+\n",
            "|0.0  |(28,[0,1,4,9,10,13,14,16,18,20,22,24,25],[29.85,29.85,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                         |\n",
            "|0.0  |(28,[0,1,2,3,4,5,6,9,11,12,15,16,18,20,26],[56.95,1889.5,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])              |\n",
            "|1.0  |(28,[0,1,2,3,4,5,6,9,11,13,14,16,18,20,22,24,26],[53.85,108.15,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
            "|0.0  |(28,[0,1,2,3,4,9,11,12,15,17,18,20,27],[42.3,1840.75,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                          |\n",
            "|1.0  |(28,[0,1,3,4,5,6,8,10,12,14,16,18,20,22,24,25],[70.7,151.65,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])       |\n",
            "+-----+----------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Train rows: 5036, Test rows: 2007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "import time\n",
        "\n",
        "# Random Forest model creation\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    numTrees=400,\n",
        "    maxDepth=16,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# spark Pipeline\n",
        "pipeline_rf = Pipeline(stages=[rf])\n",
        "\n",
        "start_time = time.time()\n",
        "print(\"Training Random Forest...\")\n",
        "\n",
        "rf_model = pipeline_rf.fit(train_df)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Training completed in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "\n",
        "print(\"Random Forest trained with {} trees.\".format(rf_model.stages[0].getNumTrees))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpdvqcH88KuG",
        "outputId": "16204439-422a-40fb-8619-724573a3b851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Random Forest...\n",
            "Training completed in 272.87 seconds.\n",
            "Random Forest trained with 400 trees.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance analysis\n",
        "\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "\n",
        "# Prediction on test set\n",
        "predictions = rf_model.transform(test_df)\n",
        "\n",
        "# Prediction results(five first rows)\n",
        "predictions.select(\"label\", \"prediction\", \"probability\").show(5, truncate=False)\n",
        "\n",
        "# Accuracy, F1-score, AUC\n",
        "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator_acc.evaluate(predictions)\n",
        "\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1 = evaluator_f1.evaluate(predictions)\n",
        "\n",
        "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "auc = evaluator_auc.evaluate(predictions)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test F1-score: {f1:.4f}\")\n",
        "print(f\"Test AUC: {auc:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "confusion = predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "confusion.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YepwG0ya_quV",
        "outputId": "52e48b90-b594-42ad-e77d-5a14c78e8f44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+----------------------------------------+\n",
            "|label|prediction|probability                             |\n",
            "+-----+----------+----------------------------------------+\n",
            "|0.0  |1.0       |[0.19423605648653466,0.8057639435134654]|\n",
            "|1.0  |0.0       |[0.5828724589587136,0.41712754104128646]|\n",
            "|0.0  |0.0       |[0.6220733355366334,0.37792666446336654]|\n",
            "|1.0  |1.0       |[0.4460315522681298,0.5539684477318703] |\n",
            "|0.0  |0.0       |[0.7248097863292838,0.2751902136707162] |\n",
            "+-----+----------+----------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Test Accuracy: 0.7982\n",
            "Test F1-score: 0.7838\n",
            "Test AUC: 0.8435\n",
            "\n",
            "Confusion Matrix:\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|  0.0|       0.0| 1357|\n",
            "|  0.0|       1.0|  113|\n",
            "|  1.0|       0.0|  292|\n",
            "|  1.0|       1.0|  245|\n",
            "+-----+----------+-----+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}